{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda语义\n",
    "`作者：Tina`\n",
    "`时间：2018-05-07`\n",
    "\n",
    "[torch-cuda](https://pytorch.org/docs/stable/cuda.html#module-torch.cuda)用于设置和运行CUDA操作。它会跟踪当前选中的GPU，而你分配的所有CUDA张量将默认在该设备上创建。选择的设备可以用[torch.cuda.device](https://pytorch.org/docs/stable/cuda.html#torch.cuda.device)上下文管理器来改变。\n",
    "\n",
    "然而，一旦一个张量被分配，你就可以对它进行操作，不管选择的设备是什么，结果总是会被放置在与张量相关的那个设备上。\n",
    "\n",
    "默认情况下，交叉gpu操作是不允许的，除了[copy_()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_)以及其他具有复制功能的方法，例如[to()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to)和[cuda()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda)。除非你启用点对点内存访问，否则任何在不同设备上操作张量的尝试都会产生错误。\n",
    "\n",
    "下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.device is:  cuda:1\n",
      "z.device is:  cuda:0\n",
      "d.device is : cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda = torch.device('cuda')     # 默认的CUDA设备\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')  # GPU 1 (these are 0-indexed)\n",
    "\n",
    "x = torch.tensor([1., 2.], device=cuda0)\n",
    "# x.device is device(type='cuda', index=0)\n",
    "y = torch.tensor([1., 2.]).cuda()\n",
    "# y.device is device(type='cuda', index=0)\n",
    "\n",
    "with torch.cuda.device(1):\n",
    "    # 在GPU1上分配一个张量\n",
    "    a = torch.tensor([1., 2.], device=cuda)\n",
    "\n",
    "    # 将一个张量从CPU转移到GPU 1\n",
    "    b = torch.tensor([1., 2.]).cuda()\n",
    "    # a.device和b.device都是(type='cuda', index=1)\n",
    "\n",
    "    # 也可以使用``Tensor.to`` 去转移一个tensor:\n",
    "    b2 = torch.tensor([1., 2.]).to(device=cuda)\n",
    "    # b.device and b2.device are device(type='cuda', index=1)\n",
    "\n",
    "    c = a + b\n",
    "    # c.device is device(type='cuda', index=1)\n",
    "    print(\"c.device is: \", c.device)\n",
    "    z = x + y\n",
    "    # z.device is device(type='cuda', index=0)\n",
    "    print(\"z.device is: \", z.device)\n",
    "\n",
    "    # 即使在一个上下文中，也可以指定设备\n",
    "    # (或者调用.cuda，参数转入一个索引)\n",
    "    d = torch.randn(2, device=cuda0)\n",
    "    e = torch.randn(2).to(cuda0)\n",
    "    f = torch.randn(2).cuda(cuda0)\n",
    "    print(\"d.device is :\", d.device)\n",
    "    # d.device, e.device, and f.device are all device(type='cuda', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异步执行(Asynchronous execution)\n",
    "默认情况下，GPU操作是异步的。当你调用一个使用GPU的函数时，操作会被排队到特定的设备上，但会在稍后执行。这允许我们并行执行更多的计算，包括在CPU或其他GPU上的操作。\n",
    "\n",
    "一般来说，异步计算的效果对调用者来说是不可见的，因为：\n",
    "- 每个设备按照操作的排队顺序去执行\n",
    "- 当在CPU和GPU之间复制数据或者在两个GPU之间复制数据时，PyTorch自动执行必要的同步。因此，计算将继续进行，就好像每个操作都是同步执行的。\n",
    "\n",
    "你可以通过设置环境变量`CUDA_LAUNCH_BLOCKING=1`来强制同步计算。当GPU上出现错误时，这是很方便的。对于异步执行，这样的错误在实际执行操作之后才会被报告，所以堆栈跟踪没有显示操作被请求的位置。\n",
    "\n",
    "作为一个例外，一些函数，例如[copy_()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_)允许一个显式的`asyns`参数，这样就可以让调用者在不必要的情况下绕过同步。另一个例外是CUDA流，下面解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA流\n",
    "[CUDA流](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams)是一个属于特定设备的线性执行序列。通常不需要显式地创建一个CUDA流，默认情况下，每个设备都使用自己的“默认”流。每条流中的操作都按照它们创建的顺序进行序列化，但是来自不同流的操作可以在任何相对顺序中并发执行，除非使用显式同步函数，比如[synchronize()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.synchronize)或者[wait_stream](https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream.wait_stream)\n",
    "\n",
    "例如，以下代码是不正确的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stream() missing 1 required positional argument: 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a64597c015e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a new stream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# sum() may start execution before normal_() finishes!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Issue 19330: ensure context manager instances have good docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stream() missing 1 required positional argument: 'stream'"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cuda')\n",
    "s = torch.cuda.stream()  # Create a new stream.\n",
    "A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\n",
    "with torch.cuda.stream(s):\n",
    "    # sum() may start execution before normal_() finishes!\n",
    "    B = torch.sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“当前流”是默认流时，当数据移动时，PyTorch会自动执行必要的同步，如上所述。然而，在使用非默认流时，应确保正确的同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内存管理\n",
    "PyTorch使用一个缓存内存分配器来加速内存分配，这允许在没有设备同步的情况下快速回收内存。然而，分配器管理的未使用内存仍将被显示，如同`nvidia-smi`那样。\n",
    "\n",
    "你可以使用[memory_allocated()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_allocated)和[max_memory_allocated()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated)监控张量所占用的内存。\n",
    "\n",
    "使用[memory_cached()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_cached)和[max_memory_cached()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_cached)来监控由缓存分配器管理的内存。\n",
    "\n",
    "调用[empty_cache()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.empty_cache)可以释放PyTorch中所有未使用的缓存内存，以便其他GPU应用程序使用这些内存。然而，张量占用的GPU内存将不会被释放，因此它不能增加用于PyTorch的GPU内存量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最佳实践\n",
    "### 设备无关代码(Device-agnostic code)\n",
    "由于PyTorch的结构，你可能需要显式地编写与设备（CPU或GPU）无关的代码。一个例子可能是创建一个新的张量作为一个循环神经网络的初始隐藏状态。\n",
    "\n",
    "第一步是确定是否应该使用GPU。一个常见的模式是使用Python的`argparse`模块读取用户参数，并用一个可以禁用CUDA的标志，与[is_available()](https://pytorch.org/docs/stable/cuda.html#torch.cuda.is_available)结合使用。下面例子中，`args.device`结果是一个可以用来将张量移动到CPU或CUDA的`torch.device`对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n",
    "# args = parser.parse_args() # 官方代码，这里有问题，修改如下\n",
    "args = parser.parse_args(['--disable-cuda'])\n",
    "args.device = None\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在有了`args.device`，我们可以用它来在想要的设备上创建一个张量。代码如下，这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty((8, 42), device=args.device)\n",
    "print(x.device)\n",
    "# net = Network().to(device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这可以在许多情况下使用，以产生设备无关的代码。下面是使用`dataloader`的一个例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cuda0 = torch.device('cuda:0')  # CUDA GPU 0\n",
    "for i, x in enumerate(train_loader):\n",
    "    x = x.to(cuda0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当在一个系统上使用多个GPU时，可以使用`CUDA_VISIBLE_DEVICES`环境标志来管理PyTorch可使用的GPU。如上所述，为了手动控制一个张量创建在哪一个GPU上，最好的做法是使用一个[torch.cuda.device](https://pytorch.org/docs/stable/cuda.html#torch.cuda.device)上下文管理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside device is 0\n",
      "Inside device is 1\n",
      "Outside device is still 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Outside device is 0\")  # On device 0 (default in most scenarios)\n",
    "with torch.cuda.device(1):\n",
    "    print(\"Inside device is 1\")  # On device 1\n",
    "print(\"Outside device is still 0\")  # On device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个张量，并且**想要在同一个设备上创造一个相同类型的新张量，那么你可以使用一个`torch.Tensor.new_*`方法**（见[torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)]）。然而前面提到的`torch.*`函数([creation-ops](https://pytorch.org/docs/stable/torch.html#tensor-creation-ops))取决于当前的GPU上下文和传入的属性参数。但`torch.Tensor.new_*`方法保存这个张量的设备和其他属性信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前向传播过程中需要在内部创建新的张量时，在创建模块时，推荐的做法是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3000,  0.3000],\n",
      "        [ 0.3000,  0.3000],\n",
      "        [ 0.3000,  0.3000]])\n",
      "tensor([[-5., -5.],\n",
      "        [-5., -5.],\n",
      "        [-5., -5.]], device='cuda:0')\n",
      "tensor([[ 1,  2,  3]])\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cuda')\n",
    "x_cpu = torch.empty(2)\n",
    "x_gpu = torch.empty(2, device=cuda)\n",
    "x_cpu_long = torch.empty(2, dtype=torch.int64)\n",
    "\n",
    "y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\n",
    "print(y_cpu)\n",
    "\n",
    "y_gpu = x_gpu.new_full([3, 2], fill_value=-5)\n",
    "print(y_gpu)\n",
    "\n",
    "y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\n",
    "print(y_cpu_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你想要创建一个与另一个张量有相同类型和尺寸的张量，并且用1或0填充它，可以使用[ones_like](https://pytorch.org/docs/stable/torch.html#torch.ones_like)和[zeros_like](https://pytorch.org/docs/stable/torch.html#torch.zeros_like)（也保存了Tensor的`torch.device`和`torch.dtype`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x_cpu = torch.empty(2, 3)\n",
    "x_gpu = torch.empty(2, 3)\n",
    "\n",
    "y_cpu = torch.ones_like(x_cpu)\n",
    "y_gpu = torch.zeros_like(x_gpu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用固定内存缓冲区\n",
    "当副本来自固定 (页锁) 内存时, 主机到 GPU 的复制速度要快很多。CPU张量和存储开放了一个[pinmemory()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.pin_memory)方法，它返回对象的副本，将数据放在一个固定的区域。\n",
    "\n",
    "另外，一旦你固定了一个张量或存储，你可以使用异步的GPU拷贝，仅仅通过一个额外的`non_blocking=True`参数传给[cude()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda)，这可以用于重叠数据传输与计算。\n",
    "\n",
    "你可以通过将`pinmemory=True`传递给它的构造器，从而使[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)将batch返回到固定的内存中。\n",
    "\n",
    "在0.1.9版本中，更大数量的GPU（8+）可能没有得到充分利用，然而, 这是一个已知的问题, 也正在积极开发中。\n",
    "\n",
    "用[multiprocessing](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing)来使用CUDA模型有很多需要注意的地方，除非小心谨慎地满足数据处理要求，你的程序很可能会有不正确或未定义的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
