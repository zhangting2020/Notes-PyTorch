{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch\n",
    "`作者：Tina`\n",
    "`时间：2018-05-09`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "- **`torch.is_tensor(obj)`**：如果`obj`是一个`PyTorch`张量，则返回`True`。\n",
    "- **`torch.is_storage(obj)`**：如果`obj`是一个`PyTorch`存储对象，则返回`True`。\n",
    "- **`torch.set_default_dtype(d)`**：将默认的浮点`dtype`设置为`d`。该类型将用作`torch.tensor()`的默认浮点类型。默认的浮点dtype初始化为`torch.float32`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor([1.2, 3]).dtype) \n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.tensor([1.2, 3]).dtype)           # 一个新的浮点张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`torch.get_default_dtype()`** → :class:`torch.dtype`：获得当前默认的浮点类型`torch.dtype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(torch.get_default_dtype())  # 初始默认类型，可能会受前面代码的影响，原本应该是float32\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.get_default_dtype())  # default is now changed to torch.float64\n",
    "torch.set_default_tensor_type(torch.FloatTensor)  # 设置张量类型也会影响这个\n",
    "print(torch.get_default_dtype())  # changed to torch.float32, the dtype for torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`torch.set_default_tensor_type(t)`**：设置默认的`torch.Tensor`类型为浮点张量类型`t`。该类型还将用作`torch.Tensor()`类型推断的默认浮点类型。默认的浮点张量类型最初是`torch.FloatTensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1.2, 3]).dtype)    # initial default for floating point is torch.float32\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(torch.tensor([1.2, 3]).dtype)    # a new floating point tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`torch.numel(input)`** → int：返回`input`张量中元素的总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 3, 4, 5)\n",
    "print(torch.numel(a))\n",
    "a = torch.zeros(4,4)\n",
    "print(torch.numel(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)`**：设置打印选项. 从 Numpy 中采集数据\n",
    "  - Parameters:\n",
    "    - `precision`：浮点输出的精度数（默认=8）。\n",
    "    - `threshold`：触发汇总显示而不是完全显示(repr)的数组元素的总数 (默认值为 1000)。\n",
    "    - `edgeitems`：在每个维度的开始和结束时的数组项目的数量（默认=3）。\n",
    "    - `linewidth`：插入换行符的每行字符数 (默认值为 80)。 Thresholded matricies(阈值矩阵) 将忽略这个参数。\n",
    "    - `profile`：用于漂亮格式的打印. 可以用以下任何选项来进行覆盖 (default, short, full)\n",
    "- **`torch.set_flush_denormal(mode)`** → bool：在CPU上禁用非正常的浮点数。如果你的系统支持刷新非正常数字并且成功配置了刷新非正常数字的模式，则返回True。只支持SSE3的x86架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.])\n",
      "tensor(9.88131e-324 *\n",
      "       [ 1.0000])\n"
     ]
    }
   ],
   "source": [
    "torch.set_flush_denormal(True)\n",
    "print(torch.tensor([1e-323], dtype=torch.float64))\n",
    "torch.set_flush_denormal(False)\n",
    "print(torch.tensor([1e-323], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建操作\n",
    "注意：随机抽样创建操作在`Random sampling`中列出。包括`torch.rand()`，`torch.rand_like()`，`torch.randn()`，`torch.randn_like()`，`torch.randint()`，`torch.randint_like()`，`torch.randperm()`。你也可以以原地操作方法使用`torch.empty()`创建从更广泛的分布范围中取样的值的`torch.Tensor()`。\n",
    "\n",
    "- **`torch.tensor(data, dtype=None, device=None, requires_grad=False)`** → Tensor：使用`data`构建一个张量。注意：`torch.tensor()`总是拷贝`data`，如果你有`data`张量，想要避免拷贝，使用`torch.Tensor.requires_grad()`或者`torch.Tensor.detach()`。如果你有一个`Numpy`的`ndarray`并且想要避免拷贝，使用`torch.from_numpy()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "torch.tensor([0, 1])  # Type inference on data\n",
    "torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                 dtype=torch.float64,\n",
    "                 device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\n",
    "torch.tensor(3.14159)  # 创建一个标量，即0维张量\n",
    "torch.tensor([])  # 创建一个空的张量(of size (0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`torch.from_numpy(ndarray)`** → Tensor：从`numpy.ndarray`类创建一个张量。返回的张量和ndarray共享相同的内存。对张量的修改将反映在ndarray中，反之亦然。返回的张量是不可调整大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3])\n",
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "a = numpy.array([1, 2, 3])\n",
    "t = torch.from_numpy(a)\n",
    "print(t)\n",
    "t[0] = -1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`torch.zeros(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`** → Tensor：返回一个填充标量值0的张量，其形状由变量参数`size`定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(2, 3))\n",
    "print(torch.zeros(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor**：返回和`input`属性相同的张量，填充为0。在PyTorch 0.4.0版本中不支持`out`关键字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.empty(2, 3)\n",
    "torch.zeros_like(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.ones(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**\n",
    "- **torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：返回一维的tensor，尺寸为$\\frac {end - start} {step}$，取值为`[start,end)`，步长为step。注意：在与`end`比较时，非整型步长受制于浮点舍入误差，为了避免不一致，我们建议在这种情况下给`end`添加一个小的epsilon。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.])\n",
      "tensor([ 1.,  2.,  3.])\n",
      "tensor([ 1.0000,  1.5000,  2.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(5))\n",
    "print(torch.arange(1, 4))\n",
    "print(torch.arange(1, 2.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：返回一个一维张量，尺寸为$\\frac {end-start} {step} + 1$，值在`[start,end]`，步长为step。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.])\n",
      "tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.range(1, 4))\n",
    "print(torch.range(1, 4, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：返回[start,end]之间等间隔`step`点的一维 Tensor。输出是尺寸为`step`的一维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n",
      "tensor([-10.,  -5.,   0.,   5.,  10.])\n",
      "tensor([-10.,  -5.,   0.,   5.,  10.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.linspace(3, 10, steps=5))\n",
    "print(torch.linspace(-10, 10, steps=5))\n",
    "print(torch.linspace(start=-10, end=10, steps=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.logspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：返回一个$[10^{start},10^{end}]$区间对数间隔的一维张量，尺寸为steps。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n",
      "tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.logspace(start=-10, end=10, steps=5))\n",
    "print(torch.logspace(start=0.1, end=1.0, steps=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.empty(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：返回一个未初始化的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-310 *\n",
       "       [[ 6.9249,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4016e+14,  3.5992e+07,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.empty((2,3), dtype=torch.int64)\n",
    "print(torch.empty_like(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1416,  3.1416,  3.1416],\n",
       "        [ 3.1416,  3.1416,  3.1416]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 3.141592)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引, 切片(Slicing), 连接(Joining), 换位(Mutating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.cat(seq, dim=0, out=None) → Tensor**：在给定维度上对输入的张量序列`seq`进行连接操作。所有张量必须具有相同的形状(除了连接的那个维度)或为空。`torch.cat()`可以看做是`torch.split()`和`torch.chunk()`的逆操作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0577, -1.5973, -0.1964],\n",
      "        [ 0.4623,  0.1471,  0.4144]])\n",
      "tensor([[ 0.0577, -1.5973, -0.1964],\n",
      "        [ 0.4623,  0.1471,  0.4144],\n",
      "        [ 0.0577, -1.5973, -0.1964],\n",
      "        [ 0.4623,  0.1471,  0.4144],\n",
      "        [ 0.0577, -1.5973, -0.1964],\n",
      "        [ 0.4623,  0.1471,  0.4144]])\n",
      "tensor([[ 0.0577, -1.5973, -0.1964,  0.0577, -1.5973, -0.1964,  0.0577,\n",
      "         -1.5973, -0.1964],\n",
      "        [ 0.4623,  0.1471,  0.4144,  0.4623,  0.1471,  0.4144,  0.4623,\n",
      "          0.1471,  0.4144]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.cat((x, x, x), 0))\n",
    "print(torch.cat((x, x, x), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.chunk(tensor, chunks, dim=0) → List of Tensors**：将一个张量分割成特定数量的块。如果在给定维度`dim`上张量大小不能被`chunks`整除，那么最后一块就会变小。其中`chunks (int)`为要返回的分块的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5464,  0.7982,  0.3336, -0.2656, -1.4394,  1.6284,  0.7265,\n",
      "          0.5419],\n",
      "        [-0.7012,  0.8646, -0.4545,  0.7369,  0.3158, -0.7613,  0.2319,\n",
      "          0.7277],\n",
      "        [-0.5963,  0.9740,  0.9107, -1.9625, -0.7183,  1.5816, -0.1496,\n",
      "         -0.0038],\n",
      "        [-0.7176,  0.9245,  0.9303, -0.4075, -2.3236, -0.2748,  0.7013,\n",
      "          0.7417]])\n",
      "(tensor([[-0.5464,  0.7982,  0.3336, -0.2656],\n",
      "        [-0.7012,  0.8646, -0.4545,  0.7369],\n",
      "        [-0.5963,  0.9740,  0.9107, -1.9625],\n",
      "        [-0.7176,  0.9245,  0.9303, -0.4075]]), tensor([[-1.4394,  1.6284,  0.7265,  0.5419],\n",
      "        [ 0.3158, -0.7613,  0.2319,  0.7277],\n",
      "        [-0.7183,  1.5816, -0.1496, -0.0038],\n",
      "        [-2.3236, -0.2748,  0.7013,  0.7417]]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,8)\n",
    "print(x)\n",
    "print(torch.chunk(x, 2, 1)) #变为4x4的两个tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.gather(input, dim, index, out=None) → Tensor**：沿给定轴dim ,将输入索引张量`index`指定位置的值进行聚合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2],\n",
      "        [ 3,  4]])\n",
      "tensor([[ 1,  1],\n",
      "        [ 4,  3]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "print(t)\n",
    "print(torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **torch.index_select(input, dim, index, out=None) → Tensor**：返回一个新的张量，它利用`index`中的项，沿着维度`dim`来对`input`张量进行索引，`index`是一个`LongTensor`。返回的张量与原始张量（`input`）的维数相同。返回的张量不与原始张量共享内存空间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7915, -0.6145,  0.4472,  2.2522],\n",
      "        [ 1.2976, -0.2220,  0.0043,  0.6490],\n",
      "        [ 1.6463, -0.0351, -1.1478, -1.2429]])\n",
      "tensor([ 0,  2])\n",
      "tensor([[ 0.7915, -0.6145,  0.4472,  2.2522],\n",
      "        [ 1.6463, -0.0351, -1.1478, -1.2429]])\n",
      "tensor([[ 0.7915,  0.4472],\n",
      "        [ 1.2976,  0.0043],\n",
      "        [ 1.6463, -1.1478]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "indices = torch.LongTensor([0, 2])\n",
    "print(indices)\n",
    "print(torch.index_select(x, 0, indices))\n",
    "print(torch.index_select(x, 1, indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.masked_select(input, mask, out=None) → Tensor**：根据掩码张量`mask`中的二元值,取输入张量中的指定项 (`mask`为一个`ByteTensor`),返回到一个新的一维张量。张量`mask`与`input`的`shape`或维度不需要相同,但是他们必须是`broadcastable`。返回的张量不与原始张量共享内存空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2825,  0.3485, -0.3233,  0.8883],\n",
      "        [ 1.0818, -0.6986,  0.4011,  0.0462],\n",
      "        [ 0.0958,  0.7562, -0.3660, -0.0232]])\n",
      "tensor([[ 0,  0,  0,  1],\n",
      "        [ 1,  0,  0,  0],\n",
      "        [ 0,  1,  0,  0]], dtype=torch.uint8)\n",
      "tensor([ 0.8883,  1.0818,  0.7562])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "mask = x.ge(0.5)\n",
    "print(mask)\n",
    "print(torch.masked_select(x, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.nonzero(input, out=None) → LongTensor**：返回一个包含输入 input 中非零元素索引的张量。输出张量中的每行包含 input 中非零元素的索引。如果输入张量 input 有 n 维,则输出的索引张量 out 的 size 为 z x n , 这里 z 是输入张量 input 中所有非零元素的个数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 1,  1],\n",
       "        [ 2,  2],\n",
       "        [ 3,  3]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.nonzero(torch.tensor([1, 1, 1, 0, 1])))\n",
    "torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.0, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.reshape(input, shape) → Tensor**：一个维度可能是-1，在这种情况下，它是从其他的维度和输入中元素的数量推断出来的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.])\n",
      "tensor([[ 0.,  1.],\n",
      "        [ 2.,  3.]])\n",
      "tensor([[ 0,  1],\n",
      "        [ 2,  3]])\n",
      "tensor([ 0,  1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(4)\n",
    "print(a)\n",
    "print(torch.reshape(a, (2, 2)))\n",
    "b = torch.tensor([[0, 1], [2, 3]])\n",
    "print(b)\n",
    "print(torch.reshape(b, (-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.split(tensor, split_size_or_sections, dim=0)**：把张量分解成块。\n",
    "  - 若`split_size_or_sections`是整型数字并且tensor可分，将被分割成同样大小的块\n",
    "  - 如果沿指定维的张量大小不能被`split_size`整分, 则最后一个分块会小于其它分块\n",
    "  - 若`split_size_or_sections`是一个list，那么tensor将被分成`len(split_size_or_sections)`个块。维度`dim`上的尺寸与`split_size_or_sections`相关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4592,  0.0379,  1.1245, -0.4108, -0.8312,  0.8239, -0.3554,\n",
      "         -1.0574],\n",
      "        [-0.7767, -0.9240, -1.1933,  1.1903,  0.1634, -1.3718,  0.9050,\n",
      "          0.3668],\n",
      "        [-1.6868, -0.7689, -2.5438,  1.3482, -0.8898, -0.8465,  0.8001,\n",
      "          0.8741],\n",
      "        [-0.6317,  0.3544, -0.1787,  1.0835, -0.5380,  0.6939, -1.2548,\n",
      "         -0.6084]])\n",
      "(tensor([[-0.4592,  0.0379],\n",
      "        [-0.7767, -0.9240],\n",
      "        [-1.6868, -0.7689],\n",
      "        [-0.6317,  0.3544]]), tensor([[ 1.1245, -0.4108],\n",
      "        [-1.1933,  1.1903],\n",
      "        [-2.5438,  1.3482],\n",
      "        [-0.1787,  1.0835]]), tensor([[-0.8312,  0.8239],\n",
      "        [ 0.1634, -1.3718],\n",
      "        [-0.8898, -0.8465],\n",
      "        [-0.5380,  0.6939]]), tensor([[-0.3554, -1.0574],\n",
      "        [ 0.9050,  0.3668],\n",
      "        [ 0.8001,  0.8741],\n",
      "        [-1.2548, -0.6084]]))\n",
      "(tensor([[ 0.6853, -0.1759, -1.4606, -0.1064],\n",
      "        [ 1.5527, -1.9168,  1.3562,  0.0212],\n",
      "        [ 0.4953, -1.3782, -1.4900, -0.1365],\n",
      "        [ 1.8437, -1.9523, -0.4135, -0.3497]]), tensor([[-0.6119, -0.4786, -2.0792,  1.2734],\n",
      "        [ 0.1373,  0.8231, -1.1859,  1.0194],\n",
      "        [-1.9929,  1.6710,  1.0069,  0.9423],\n",
      "        [-0.3226, -0.5149, -0.2459,  0.6075]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 8)\n",
    "print(a)\n",
    "print(torch.split(a, 2, 1)) #划分后每个块的1号维度大小为2.\n",
    "\n",
    "print(torch.chunk(x, 2, 1)) #划分后，第1维是2个块，最后得到4x4的两个tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.squeeze(input, dim=None, out=None) → Tensor**：将 input 张量 size 中的 1 去除并返回。\n",
    "  - 作为上述的一个例外,size 为 1 的一维张量不会改变维度。\n",
    "  - 如果指定了维度，那么这个挤压操作将只在给定的维度进行\n",
    "  - 返回张量与输入张量共享内存,所以改变其中一个的内容会改变另一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "print(x.size())\n",
    "y = torch.squeeze(x)\n",
    "print(y.size())\n",
    "y = torch.squeeze(x, 0) #在第1维进行挤压操作\n",
    "print( y.size())\n",
    "y = torch.squeeze(x, 1)#在第2维进行挤压操作\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.stack(seq, dim=0, out=None) → Tensor**：在一个新的维度上连接张量序列。所有的张量都要有相同的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  1.,  1.],\n",
      "         [ 0.,  0.,  0.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.],\n",
      "         [ 0.,  0.,  0.]]])\n",
      "tensor([[[ 1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.]]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones(2, 3)\n",
    "a = torch.zeros(2, 3)\n",
    "print(torch.stack((b, a), 1))\n",
    "print(torch.stack((b, a, b), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.t(input, out=None) → Tensor**：希望 input 为一个矩阵 (2 维张量), 并转置 0, 1 维。可以被视为函数 transpose(input, 0, 1) 的简写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0355,  0.1632],\n",
      "        [-1.1098,  0.0926],\n",
      "        [-2.0556,  1.6433]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(torch.t(x)) #->size = (3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.take(input, indices) → Tensor**：在给定的索引上返回一个带有输入元素的新张量。input 张量被看作是一个1-D张量。返回的张量形状与`indices`相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  5,  8])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor([[4, 3, 5],\n",
    "                        [6, 7, 8]])\n",
    "print(torch.take(src, torch.tensor([0, 2, 5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.transpose(input, dim0, dim1, out=None) → Tensor**：返回转置。dim0 和 dim1交换。out 张量与 input 张量共享内存,所以改变其中一个会导致另外一个也被修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5680,  0.6735, -1.1688],\n",
      "        [-1.8213,  0.2811, -0.4789]])\n",
      "tensor([[-1.5680, -1.8213],\n",
      "        [ 0.6735,  0.2811],\n",
      "        [-1.1688, -0.4789]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.transpose(x, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.unbind(tensor, dim=0)**：删除一个张量维度。移除指定维后,返回一个元组,包含了沿着指定维度切片后的各个切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3070, -0.6959, -0.0401],\n",
      "        [ 0.0065, -0.8692, -2.0186]])\n",
      "(tensor([-0.3070, -0.6959, -0.0401]), tensor([ 0.0065, -0.8692, -2.0186]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.unbind(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.unsqueeze(input, dim, out=None) → Tensor**：返回在指定位置插入维度 size 为 1 的新张量。返回的张量与这个张量共享相同的底层数据（个人理解：指的是数据内容都是一样的，但是形态可能不一样，比如形状）。\n",
    "如果 dim 为负，范围为 [-input.dim(), input.dim() ) ，则将会被转化 dim+input.dim()+1 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([ 1,  2,  3,  4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([4])\n",
      "tensor([[ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [ 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3, 4]) #一维\n",
    "print(x.size())\n",
    "print(x)\n",
    "\n",
    "y = torch.unsqueeze(x, 0) #插入一个为1的维度，变为二维\n",
    "print(y.size())\n",
    "print(x.size())\n",
    "print(torch.unsqueeze(x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.where(condition, x, y) → Tensor**：返回一个张量，其中的元素是从 x 或者 y 中选择而来的，取决于`condition`。定义为：\n",
    "$$\n",
    "out_i =\n",
    " \\begin{cases}\n",
    "   x_i &\\mbox{if condition}\\\\\n",
    "   y_i &\\mbox{otherwise}\n",
    "   \\end{cases}\n",
    "$$\n",
    "其中condition，x，y张量都必须是可扩展的（broadcastable）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0715, -0.3459],\n",
      "        [-0.0258, -0.4257],\n",
      "        [-0.3096, -0.4965]])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2)\n",
    "print(x)\n",
    "y = torch.ones(3, 2)\n",
    "print(y)\n",
    "print(torch.where(x > 0, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机采样(Random sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.manual_seed(seed)**：设置生成随机数的种子。返回一个 torch._C.Generator 对象\n",
    "- **torch.initial_seed()**：返回用于生成随机数字的初始种子 (python long) \n",
    "- **torch.get_rng_state()**：以ByteTensor的形式返回随机数发生器的状态\n",
    "- **torch.set_rng_state(new_state)**：设置随机数生成器状态。new_state (torch.ByteTensor) – The desired state\n",
    "- **torch.default_generator = <torch._C.Generator object>**\n",
    "- **torch.bernoulli(input, out=None) → Tensor**：从伯努利分布中抽取二值随机数（0或1）。input 张量包含用于抽取随机数的概率。 因此, input 中的所有值必须在[0, 1]范围。\n",
    "  - 根据 input 张量第 i 个概率值, 输出张量的第 i 个元素将取值为1\n",
    "  - 返回的张量只有0或1的值，和输入的形状是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0076,  0.7598,  0.8647],\n",
      "        [ 0.9484,  0.7426,  0.6615],\n",
      "        [ 0.3149,  0.8622,  0.4720]])\n",
      "tensor([[ 0.,  0.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 0.,  1.,  0.]])\n",
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]])\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1]\n",
    "print(a)\n",
    "print(torch.bernoulli(a))\n",
    "a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n",
    "print(torch.bernoulli(a))\n",
    "a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n",
    "print(torch.bernoulli(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.multinomial(input, num_samples, replacement=False, out=None) → LongTensor**：返回一个张量, 其中每一行包含在 input 张量对应行中多项式分布取样的 num_samples 个索引。input中包含了概率值\n",
    "\n",
    "注意：input 的每行值不需要总和为 1 (我们只使用这些值作为权重), 但必须是非负且非零和的。\n",
    "  - 取样时从左向右排列(第一个样本在第一列)\n",
    "  - 如果 input 是一个向量, 则 out 是一个大小为 num_samples 的向量\n",
    "  - 如果 input 是一个 m 行的矩阵, 则 out 是一个 m × n 的矩阵\n",
    "  - 如果参数 replacement 是 True, 则可重复取样。否则, 样本在每行不能被重复取样\n",
    "  - 参数 num_samples 必须小于 input 长度 (如果是一个矩阵, 则是 input 的列数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,  10.,   3.,   0.], dtype=torch.float32)\n",
      "tensor([ 1,  2,  0,  0])\n",
      "tensor([ 1,  1,  2,  1])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n",
    "print(weights)\n",
    "print(torch.multinomial(weights, 4))\n",
    "print(torch.multinomial(weights, 4, replacement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.normal()**\n",
    "  - torch.normal(mean, std, out=None) → Tensor：返回一个离散正态分布所产生的随机数张量。中 means 和 std 的形状不需要匹配, 但是每个张量中的元素总数需要相同。当形状不匹配时, means 的形状将作为返回输出张量的形状。\n",
    "  -  torch.normal(mean=0.0, std, out=None) → Tensor：功能与上面函数类似, 但所有被抽取的元素共享均值\n",
    "  - torch.normal(mean, std=1.0, out=None) → Tensor：功能与上面函数类似, 但所有被抽取的元素共享标准差\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1337,  1.0700,  5.2551,  3.4929,  4.7147])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))\n",
    "\n",
    "torch.normal(mean=0.5, std=torch.arange(1, 6))\n",
    "\n",
    "torch.normal(mean=torch.arange(1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **torch.rand(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：在区间 [0,1) 中, 返回一个填充了均匀分布的随机数的张量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0191,  0.9621,  0.8359,  0.2107])\n",
      "tensor([[ 0.7785,  0.8355,  0.3480],\n",
      "        [ 0.6659,  0.0195,  0.7441]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(4))\n",
    "print(torch.rand(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor**\n",
    "- **torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**\n",
    "- **torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  4.,  4.])\n",
      "tensor([[ 7.,  7.],\n",
      "        [ 6.,  7.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randint(3, 5, (3,)))\n",
    "print(torch.randint(3, 10, (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.randn(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor**：返回一个从正态分布产生的随机数的张量, 其均值为 0 , 方差为 1\n",
    "- **torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6806,  1.7512,  0.6484,  0.3091])\n",
      "tensor([[ 2.2059,  0.1022, -0.1442],\n",
      "        [-0.4446, -0.9081,  1.1556]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(4))\n",
    "print(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor**：返回一个从 0 到 n-1 的随机的整数排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  3,  1,  2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机采样的原地操作(In-place random sampling)\n",
    "还有一些在张量上定义的随机抽样函数。\n",
    "- torch.Tensor.bernoulli_()\n",
    "- torch.Tensor.cauchy_() - cauchy分布\n",
    "- torch.Tensor.exponential_()\n",
    "- torch.Tensor.geometric_()\n",
    "- torch.Tensor.log_normal_()\n",
    "- torch.Tensor.normal_()\n",
    "- torch.Tensor.random_()\n",
    "- torch.Tensor.uniform_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列化(Serialization)\n",
    "\n",
    "`torch.save(obj, f, pickle_module=<module 'pickle' from '/private/home/soumith/anaconda3/lib/python3.6/pickle.py'>, pickle_protocol=2)`将一个对象保存到磁盘文件中。参见[Recommended approach for saving a model](../介绍/7.序列化语义.ipynb)\n",
    "- obj: 要保存的对象 \n",
    "- f: 类文件对象 (必须实现返回文件描述符的 fileno 方法) 或包含文件名的字符串 \n",
    "- pickle_module: 用于 pickling 元数据(metadata )和对象的模块 \n",
    "- pickle_protocol: 可以指定覆盖默认协议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "# Save to file\n",
    "x = torch.tensor([0, 1, 2, 3, 4])\n",
    "torch.save(x, 'tensor.pt') #在当前目录下可以看到一个tensor.pt的文件\n",
    "# Save to io.BytesIO buffer\n",
    "buffer = io.BytesIO()\n",
    "torch.save(x, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.load(f, map_location=None, pickle_module=<module 'pickle' from '/private/home/soumith/anaconda3/lib/python3.6/pickle.py'>)从磁盘文件中加载一个用 torch.save() 保存的对象。\n",
    "\n",
    "torch.load 使用 Python 的解封 (unpickling) 设施, 但特殊对待张量下的存储 (storages)。\n",
    "它们首先在 CPU 上反序列化，然后移动到所保存的设备上。如果这个过程失败了 (例如, 因为运行时的系统没有确定的设备), 将会抛出异常。然而, 使用 map_location 参数，存储可以被动态地重新映射到另一组设备上。\n",
    "\n",
    "如果 map_location 是可调用对象，则对于每个序列化存储，它都将以两个参数调用一次：storage 和 location。参数 storage 是驻留在 CPU 上的存储的初始反序列化。 每个序列化后的存储都有一个与之关联的位置标签，它标识了保存它的设备， 而此标签是传递给 map_location 的第二个参数。 对于 CPU 张量， 内建的位置标签是 ‘cpu’， 对于 CUDA 张量， 内建的位置标签是 ‘cuda:device_id’ (例如 ‘cuda:2’)。 map_location 要么返回 None ， 要么返回一个存储。 如果 map_location 返回存储， 它将用作已移动到正确设备上的， 最终反序列化的对象。否则， 如果没有指明 map_location， 即返回 None， torch.load 会回落到默认的行为。\n",
    "\n",
    "如果 map_location 是一个字典， 它将把出现在文件（键）中的位置标签， 重新映射到指定的存储中。\n",
    "\n",
    "用户扩展可以使用 register_package 来注册他们自己的位置标签， 以及标记和反序列化方法。\n",
    "\n",
    "- f: 一个类文件对象 (必须实现返回文件描述符的 fileno, 以及 seek 方法)，或者包含文件名的字符串\n",
    "- map_location: 一个函数或者一个指明如何重新映射存储位置的字典 \n",
    "- pickle_module: 用于解封 (unpickling) 元数据和对象的模块 (必须匹配用于序列化文件的 pickle_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('tensor.pt')\n",
    "# Load all tensors onto the CPU\n",
    "torch.load('tensor.pt', map_location=lambda storage, loc: storage)\n",
    "# Load all tensors onto GPU 1\n",
    "torch.load('tensor.pt', map_location=lambda storage, loc: storage.cuda(1))\n",
    "# Map tensors from GPU 1 to GPU 0\n",
    "torch.load('tensor.pt', map_location={'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行化\n",
    "- **torch.get_num_threads() → int**：获取用于并行化CPU操作的OpenMP线程的数量 \n",
    "- **torch.set_num_threads(int)**：设置用于并行化CPU操作的OpenMP线程的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 禁用局部梯度计算\n",
    "上下文管理器`torch.no_grad()`和`torch.enable_grad()`以及`torch.set_grad_enabled()`用来禁用和启用局部梯度计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "     y = x * 2\n",
    "print(y.requires_grad)\n",
    "is_train = False\n",
    "with torch.set_grad_enabled(is_train):\n",
    "     y = x * 2\n",
    "print(y.requires_grad)\n",
    "\n",
    "torch.set_grad_enabled(True)  # this can also be used as a function\n",
    "y = x * 2\n",
    "print(y.requires_grad)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "y = x * 2\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学操作\n",
    "### 逐点运算(Pointwise Ops)\n",
    "- **torch.abs(input, out=None) → Tensor**\n",
    "- **torch.acos(input, out=None) → Tensor**：返回反余弦值$out_i = cos^{-1}(input_i)$\n",
    "- **torch.add()**：\n",
    "  - torch.add(input, value, out=None)：每一个元素都加上一个值\n",
    "  - torch.add(input, value=1, other, out=None)：张量 other 的每个元素乘以标量值 value 并加到张量 input 上， 返回生成的张量 out。张量 input 的形状与张量 other 的形状必须 broadcastable。$out = input + (other * value)$。如果张量 other 是 FloatTensor 或者 DoubleTensor 类型, 则 value 必须为实数, 否则为整数.\n",
    "- **torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor**：$out_i = tensor_i + value \\times \\frac {tensor1_i} {tensor2_i}$。张量 tensor, 张量 tensor1, 张量 tensor2 的形状必须 broadcastable。对于类型为 FloatTensor 或者 DoubleTensor 的张量输，value 必须为实数，否则为整数。\n",
    "- **torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor**：$out_i = tensor_i + value \\times tensor1_i + tensor2_i$。张量 tensor, 张量 tensor1, 张量 tensor2 的形状必须 broadcastable。对于类型为 FloatTensor 或者 DoubleTensor 的张量输入，value 必须为实数，否则为整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3250, -0.8534,  0.7035],\n",
       "        [ 0.2746, -0.8107,  0.8232],\n",
       "        [ 0.2831, -0.8179,  0.8030]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(1, 3)\n",
    "t1 = torch.randn(3, 1)\n",
    "t2 = torch.randn(1, 3)\n",
    "torch.addcmul(t, 0.1, t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.asin(input, out=None) → Tensor**：$out_i = sin^{-1}(input_i)$\n",
    "- **torch.atan(input, out=None) → Tensor**：$out_i = tan^{-1}(input_i)$\n",
    "- **torch.atan2(input1, input2, out=None) → Tensor**：$out_i = tan^{-1}(input_i)$\n",
    "- **torch.ceil(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是张量 input 的元素向上取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3303,  0.4434, -0.6123,  1.2916])\n",
      "tensor([-0.,  1., -0.,  2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4)\n",
    "print(a)\n",
    "print(torch.ceil(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.clamp(input, min, max, out=None) → Tensor**：\n",
    " $$y_i = \\begin{cases}\n",
    "  min &\\mbox{if $x_i$ < min}\\\\\n",
    "  x_i &\\mbox{if min $\\le x_i \\le max$}\\\\\n",
    "  max &\\mbox{if $x_i$ > max}\n",
    "  \\end{cases}\n",
    " $$\n",
    "  - torch.clamp(input, *, min, out=None) → Tensor：张量 input 的所有元素值大于或者等于 min\n",
    "  - torch.clamp(input, *, max, out=None) → Tensor：张量 input 的所有元素值小于或者等于 max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.cos(input, out=None) → Tensor**\n",
    "- **torch.cosh(input, out=None) → Tensor**\n",
    "- **torch.div()**\n",
    "  - torch.div(input, value, out=None) → Tensor：每个值都除以value\n",
    "  - torch.div(input, other, out=None) → Tensor\n",
    "- **torch.erf(tensor, out=None) → Tensor**：计算每个元素的误差函数，定义为$$erf(x) = \\frac {2} {\\sqrt{\\pi}} \\int_0^{\\pi} e^{-t^2} {\\rm d}x$$\n",
    "- **torch.erfinv(tensor, out=None) → Tensor**：计算每个元素的逆误差函数\n",
    "- **torch.exp(tensor, out=None) → Tensor**：$y_i = e^{x_i}$\n",
    "- **torch.expm1(tensor, out=None) → Tensor**：$y_i = e^{x_i}-1$\n",
    "- **torch.floor(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是张量 input 的元素向下取整\n",
    "- **torch.fmod(input, divisor, out=None) → Tensor**：计算余数\n",
    "- **torch.frac(tensor, out=None) → Tensor**：计算张量 tensor 每个元素的分数部分，比如2.5对应的结果就是0.5\n",
    "- **torch.lerp(start, end, weight, out=None)**：基于标量值 weight, 在张量 start 与 end 之间做线性插值 并返回结果张量 out。张量 start 和张量 end 的形状必须可 broadcastable. $$out_i = start_i + weight * (end_i - start_i)$$\n",
    "- **torch.log(input, out=None) → Tensor**：自然对数\n",
    "- **torch.log10(input, out=None) → Tensor**\n",
    "- **torch.log1p(input, out=None) → Tensor**：$y_i = log_e(x_i+1)$\n",
    "- **torch.log2(input, out=None) → Tensor**：以2为底的对数\n",
    "- **torch.mul()**\n",
    "  - torch.mul(input, value, out=None)：$out_i = input_i * value$\n",
    "  - torch.mul(input, other, out=None)：out_i = input_i * other_i，张量 input 和张量 other 的形状必须是 broadcastable\n",
    "- **torch.neg(input, out=None) → Tensor**：取负数\n",
    "- **torch.pow()**\n",
    "  - torch.pow(input, exponent, out=None) → Tensor：对输入张量 input 按元素求 exponent 次幂值并返回结果张量(其值作为结果张量的元素)。幂值 exponent 可以是一个单一的浮点数 float 或者是一个与张量 input 有相同元素数的张量 Tensor。如果是张量，执行逐点运算\n",
    "  - torch.pow(base, input, out=None) → Tensor：base 是一个标量浮点值, input 是一个张量. 返回的张量 out 的形状与张量 input 的形状相同。\n",
    "  $out_i = base^{input_i}$\n",
    "- **torch.reciprocal(input, out=None) → Tensor**：返回元素倒数的新张量\n",
    "- **torch.remainder(input, divisor, out=None) → Tensor**：计算元素的除法的余数\n",
    "- **torch.round(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是输入张量的元素四舍五入到最近的整数\n",
    "- **torch.rsqrt(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是张量 input 元素的平方根的倒数\n",
    "- **torch.sigmoid(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是张量 input 元素的sigmoid值\n",
    "- **torch.sign(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是张量 input 元素的符号：1或-1\n",
    "- **torch.sin(input, out=None) → Tensor**\n",
    "- **torch.sinh(input, out=None) → Tensor**\n",
    "- **torch.sqrt(input, out=None) → Tensor**\n",
    "- **torch.tan(input, out=None) → Tensor**\n",
    "- **torch.tanh(input, out=None) → Tensor**\n",
    "- **torch.trunc(input, out=None) → Tensor**：返回一个新的张量 Tensor , 其元素是张量 input 元素的截断整数值 (直接去除小数部分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归约操作(Reduction Ops)\n",
    "- **torch.argmax(input, dim=None, keepdim=False)**：在维度dim上返回一个张量的最大值的索引。keepdim - 输出的张量保留还是不保留维度。\n",
    "- **torch.argmin(input, dim=None, keepdim=False)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6843, -0.1722, -0.1232, -0.5236],\n",
      "        [ 0.0536, -0.4813,  1.7423,  0.1343],\n",
      "        [-1.2162,  1.0545,  0.2850, -0.6378],\n",
      "        [-1.1819, -0.1568,  1.1632,  0.3593]])\n",
      "tensor([ 0,  2,  1,  2])\n",
      "tensor([[ 0],\n",
      "        [ 2],\n",
      "        [ 1],\n",
      "        [ 2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "print(torch.argmax(a, dim=1))\n",
    "print(torch.argmax(a, dim=1, keepdim = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.cumprod(input, dim, out=None) → Tensor**：返回元素在给定维度下的累积积。$y_i = x_1 \\times x_2 \\times...\\times x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2139, -1.4861,  0.8621,  1.2909,  0.3721, -0.2437, -2.2437,\n",
      "         1.0824,  0.3550,  0.3152])\n",
      "tensor([-1.2139,  1.8039,  1.5552,  2.0077,  0.7470, -0.1821,  0.4085,\n",
      "         0.4422,  0.1570,  0.0495])\n",
      "tensor([-1.2139,  1.8039,  1.5552,  2.0077,  0.7470,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(10)\n",
    "print(a)\n",
    "print(torch.cumprod(a, dim=0))\n",
    "a[5] = 0.0\n",
    "print(torch.cumprod(a, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.cumsum(input, dim, out=None) → Tensor**：返回元素 input 在给定维度 dim 下的累积和 $y_i = x_1 + x_2 +...+x_i$\n",
    "- **torch.dist(input, other, p=2) → Tensor**：返回(input - other)的p-范数 input 和 other 的形状必须满足 broadcastable\n",
    "- **torch.mean()**\n",
    "  - torch.mean(input) → Tensor：返回张量 input 所有元素的均值\n",
    "  - torch.mean(input, dim, keepdim=False, out=None) → Tensor：返回张量 input 在给定维度 dim 上每行的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4393, -0.4599, -0.0665, -0.2214],\n",
      "        [-0.1057,  1.5003,  1.1166, -0.3793],\n",
      "        [ 1.6394, -0.5347, -1.1903, -1.0985],\n",
      "        [ 0.4064, -2.0709, -2.1749,  1.0044]])\n",
      "tensor([-0.2968,  0.5330, -0.2960, -0.7087])\n",
      "tensor([[-0.2968],\n",
      "        [ 0.5330],\n",
      "        [-0.2960],\n",
      "        [-0.7087]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "print(torch.mean(a, 1))\n",
    "print(torch.mean(a, 1, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.median()**\n",
    "  - torch.median(input) → Tensor：返回输出张量 input 所有元素的中位数\n",
    "  - torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor)：返回输出张量 input 在给定维度 dim 下每行的中位数. 同时返回一个包含中位数的索引 LongTensor。dim 的缺省值为输入张量 input 的最后一维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1060,  0.3104, -0.1515, -1.6107, -1.7563],\n",
      "        [-0.2952, -0.2069, -0.1103, -2.3826,  1.3704],\n",
      "        [ 0.3814, -0.5650, -0.0999, -0.3361, -0.1016],\n",
      "        [ 0.3794,  0.5855, -0.0059, -0.8481, -0.4961]])\n",
      "(tensor([-1.1060, -0.2069, -0.1016, -0.0059]), tensor([ 0,  1,  4,  2]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 5)\n",
    "print(a)\n",
    "print(torch.median(a, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -> (Tensor, LongTensor)**：返回输入张量 input 在给定维数 dim 下每行元素的众数值。同时也返回众数值的索引为一个LongTensor。这个函数至今没有为 torch.cuda.Tensor 定义\n",
    "- **torch.norm()**\n",
    "  - torch.norm(input, p=2) → Tensor：返回输入张量 input 的p-范数\n",
    "  - torch.norm(input, p, dim, keepdim=False, out=None) → Tensor：返回输入张量 input 在给定维度 dim 下每行元素的p-范数\n",
    "- **torch.prod()**\n",
    "  - torch.prod(input) → Tensor：返回输入张量 input 所有元素的乘积\n",
    "  - torch.prod(input, dim, keepdim=False, out=None) → Tensor：返回输入张量 input 在给定维度 dim 下每行元素的积\n",
    "- **torch.std()**\n",
    "  - torch.std(input, unbiased=True) → Tensor：返回输入张量 input 所有元素的标准差。如果 unbiased 是 False , 那么标准差将通过有偏估计计算.否则, Bessel’s correction 将被使用\n",
    "  - torch.std(input, dim, keepdim=False, unbiased=True, out=None) → Tensor：返回输入张量 input 在给定维度 dim 下每行元素的标准差。\n",
    "- **torch.sum()**\n",
    "  - torch.sum(input) → Tensor：返回输入张量 input 所有元素的和\n",
    "  - torch.sum(input, dim, keepdim=False, out=None) → Tensor：返回输入张量 input 在给定维度 dim 下每行元素的和\n",
    "- **torch.unique(input, sorted=False, return_inverse=False)**：返回一个一维张量，每个元素是输入张量的唯一的标量元素.\n",
    "  - sorted：是否在返回输出之前按升序对唯一元素进行排序\n",
    "  - return_inverse (bool)：是否也返回原始输入中元素在输出中的位置的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  3,  1])\n",
      "tensor([ 1,  2,  3]) tensor([ 0,  2,  1,  2])\n",
      "tensor([ 1,  2,  3]) tensor([[ 0,  2],\n",
      "        [ 1,  2]])\n"
     ]
    }
   ],
   "source": [
    "output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n",
    "print(output)\n",
    "output, inverse_indices = torch.unique(\n",
    "        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n",
    "print(output, inverse_indices)\n",
    "\n",
    "output, inverse_indices = torch.unique(\n",
    "        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n",
    "print(output, inverse_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.var()**\n",
    "  - torch.var(input, unbiased=True) → Tensor：返回输入张量中所有元素的方差，如果 unbiased 是 False , 方差的计算将通过有偏估计计算。否则, Bessel’s correction 将会被使用。\n",
    "  - torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor：返回输入张量 input 在给定维度 dim 下每行的方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较操作(Comparison Ops)\n",
    "- **torch.eq(input, other, out=None) → Tensor**：比较元素是否相等。other可以使一个数字或者可扩展的张量，以方便与input比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0],\n",
       "        [ 0,  1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.equal(tensor1, tensor2) → bool**：如果两个张量有相同的形状和元素值, 则返回 True , 否则 False\n",
    "- **torch.ge(input, other, out=None) → Tensor**：逐元素比较 input 和 other , 即是否 input>=other\n",
    "- **torch.gt(input, other, out=None) → Tensor**：逐元素比较 input 和 other , 即是否 input>other \n",
    "- **torch.isnan(tensor)**\n",
    "- **torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -> (Tensor, LongTensor)**：取输入张量 input 指定维上第 k 个最小值. 如果不指定 dim , 则默认为 input 的最后一维。返回一个元组 (values,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.]])\n",
      "(tensor([[ 4.,  5.,  6.]]), tensor([[ 1,  1,  1]]))\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(1,7).resize_(2,3)\n",
    "print(x)\n",
    "print(torch.kthvalue(x,2,0,True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.le(input, other, out=None) → Tensor**：逐元素比较 input 和 other , 即是否 input<=other \n",
    "- **torch.lt(input, other, out=None) → Tensor**：逐元素比较 input 和 other , 即是否 input<other\n",
    "- **torch.max()**\n",
    "  - torch.max(input) → Tensor：返回输入 input 张量所有元素的最大值\n",
    "  - torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)：回输入张量 input 在给定维度 dim 上每行的最大值, 并同时返回每个最大值的位置索引\n",
    "  - torch.max(input, other, out=None) → Tensor：输入 input 每一个元素和对应的比较张量 other 进行比较, 留下较大的元素 max\n",
    "- **torch.min()**\n",
    "  - torch.min(input) → Tensor\n",
    "  - torch.min(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\n",
    "  - torch.min(input, other, out=None) → Tensor\n",
    "- **torch.ne(input, other, out=None) → Tensor**：逐元素比较 input 和 other , 即是否 tensor != other\n",
    "- **torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor)**：对输入张量 input 沿着指定维按升序排序，如果不给定 dim ,则默认为输入的最后一维。若descending = True，则按降序排序。返回元组 (sorted_tensor, sorted_indices)\n",
    "- **torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor)**：沿给定 dim 维度返回输入张量 input 中 k 个最大值. 如果不指定 dim , 则默认为 input 的最后一维. 如果为 largest 为 False ,则返回最小的 k 个值. 返回一个元组 (values, indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 频谱操作(Spectral Ops)\n",
    "\n",
    "待完善，[详细内容](https://pytorch.org/docs/stable/torch.html#spectral-ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他操作\n",
    "- **torch.cross(input, other, dim=-1, out=None) → Tensor**：返回沿着维度 dim 上, 两个张量 input 和 other 的向量积 (叉积), input 和 other 必须有相同的形状, 且维度的 size 必须为 3。如果不指定 dim, 则默认为第一个 size 为 3 的维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  1.],\n",
      "        [ 1.,  2.,  1.]])\n",
      "tensor([[ 1.,  0.,  1.],\n",
      "        [ 1.,  2.,  1.]])\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 0, 1], [1, 2, 1]])\n",
    "print(a)\n",
    "b = torch.Tensor([[1, 0, 1], [1, 2, 1]])\n",
    "print(b)\n",
    "print(torch.cross(a, b, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.diag(input, diagonal=0, out=None) → Tensor**\n",
    "  - 如果输入是一个向量( 1D 张量), 则返回一个以 input 为对角线元素的 2D 方阵\n",
    "  - 如果输入是一个矩阵( 2D 张量), 则返回一个包含 input 对角线元素的 1D 张量\n",
    "  \n",
    "  参数 diagonal 指定对角线：\n",
    "  - diagonal = 0, 主对角线\n",
    "  - diagonal > 0, 主对角线之上\n",
    "  - diagonal < 0, 主对角线之下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2684,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.8670,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.1462]])\n",
      "tensor([[ 0.0000,  1.2684,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.8670,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.1462],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([-0.1586, -1.1057, -0.6887])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3)\n",
    "print(torch.diag(a))\n",
    "print(torch.diag(a, 1))\n",
    "\n",
    "a = torch.randn(3, 3)\n",
    "print(torch.diag(a, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.diagonal()**：总是返回输入的对角线\n",
    "- **torch.diagflat()**：总是构造一个由输入指定的对角元素的张量\n",
    "- **torch.diagflat(input, diagonal=0) → Tensor**\n",
    "  - 如果输入是一个向量( 1D 张量), 则返回一个以 input 为对角线元素的 2D 方阵\n",
    "  - 如果输入是多于1维的tensor，然后返回一个二维张量，对角线元素等于一个输入的平铺"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2756,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.2678,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.2723]])\n",
      "tensor([[-1.3904,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.7878,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.6394,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.2856]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3)\n",
    "print(torch.diagflat(a))\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "print(torch.diagflat(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.diagonal(input, offset=0) → Tensor**：返回一个带有对角元素的1-D张量。offset指定对角线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2760, -0.1163, -0.7487],\n",
      "        [ 0.3729, -1.0589,  0.8631],\n",
      "        [-1.3717, -0.9097, -0.9015]])\n",
      "tensor([-2.2760, -1.0589, -0.9015])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 3)\n",
    "print(a)\n",
    "print(torch.diagonal(a, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.einsum(equation, operands) → Tensor**：这个函数使用爱因斯坦求和约定提供了一种计算多线性表达式的方法（比如，乘积的和）\n",
    "- **torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor**：计算张量的直方图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  1.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.renorm(input, p, dim, maxnorm, out=None) → Tensor**：返回一个张量, 包含规范化后的各个子张量, 使得沿着 dim 维划分的各子张量的 p 范数小于 maxnorm。如果一行的范数低于maxnorm，那一行就没有变化\n",
    "- **torch.trace(input) → float**：返回输入 2 维矩阵对角线元素的和(迹)\n",
    "- **torch.tril(input, diagonal=0, out=None) → Tensor**：返回一个张量, 包含输入矩阵 ( 2D 张量)的下三角部分, 其余部分被设为 0\n",
    "- **torch.triu(input, diagonal=0, out=None) → Tensor**：返回一个张量, 包含输入矩阵 ( 2D 张量)的上三角部分, 其余部分被设为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
