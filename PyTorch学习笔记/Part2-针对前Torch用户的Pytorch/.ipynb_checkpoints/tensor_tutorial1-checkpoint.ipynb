{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "PyTorch中的Tensors与Torch中的几乎是一样的。\n",
    "\n",
    "用未初始化的内存创建一个大小（5 x 7）的张量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.empty(5, 7, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个具有正态分布的double型张量，均值为0，方差为1：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2900, -1.4229, -2.7230, -1.3463, -0.4546,  0.3415, -0.2096],\n",
      "        [-1.6002,  0.3603,  0.0391,  1.0323,  1.0186,  0.5080,  0.7860],\n",
      "        [ 0.0033, -0.5321,  0.8231, -1.9239, -1.6096,  0.9697, -0.2321],\n",
      "        [-0.8466, -1.6952,  0.1851, -1.2134,  1.0681, -1.2139,  0.5348],\n",
      "        [-0.5398,  1.6228, -0.0278,  0.0599,  0.3209,  0.8887,  1.5830]], dtype=torch.float64)\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7, dtype=torch.double)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size``是一个元组，所以支持同样的操作</p></div>\n",
    "\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "第一个区别是，在tensor上的原地操作（inplace）都有一个 ``_`` 后缀。比如，``add``是非原地的操作（out-of-place），而``add_`` 是原地操作。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "        [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]], dtype=torch.float64) tensor([[ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "        [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些操作比如``narrow``没有inplace版本,有一些操作没有out-of-place版本，例如``fill_`` 。所以``.fill``不存在。\n",
    "\n",
    "零索引\n",
    "-------------\n",
    "\n",
    "另一个区别是张量是零索引的 (在lua中tensors索引开始为1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3]  # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors也可以使用Python得切片（slicing）索引。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5]  # 所有行，4-5列。注意：左包含，右不包含"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无大小写\n",
    "---------------\n",
    "\n",
    "所有的函数不再是大小写拼法。比如``indexAdd``现在改为``index_add_``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.],\n",
      "        [  10.,  100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 501.,    1.,    1.,    1.,   51.],\n",
      "        [ 501.,    1.,    1.,    1.,   51.],\n",
      "        [ 501.,    1.,    1.,    1.,   51.],\n",
      "        [ 501.,    1.,    1.,    1.,   51.],\n",
      "        [ 501.,    1.,    1.,    1.,   51.]])\n",
      "tensor([ 4,  0])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.tensor([4, 0], dtype=torch.long), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy桥\n",
    "------------\n",
    "\n",
    "- 将一个 Torch Tensor转换成一个NumPy矩阵，反之亦然。\n",
    "- Torch张量和NumPy矩阵将共享它们的底层内存位置，改变一个则会改变另一个。\n",
    "- CPU上所有的张量，除了char张量外都支持NumPy和Tensor之间的相互转换。\n",
    "\n",
    "## 将一个 Torch Tensor转换成一个NumPy数组\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  2.,  2.,  2.,  2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) \t# see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将一个NumPy数组转换成一个Torch Tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上所有的Tensor除了CharTensor都支持与Numpy的互相转换\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "将一个CUDA张量从CPU转移到GPU上，将保留它的底层类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # creates a LongTensor and transfers it\n",
    "    # to GPU as torch.cuda.LongTensor\n",
    "    a = torch.full((10,), 3, device=torch.device(\"cuda\"))\n",
    "    print(type(a))\n",
    "    b = a.to(torch.device(\"cpu\"))\n",
    "    # transfers it to CPU, back to\n",
    "    # being a torch.LongTensor\n",
    "    print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
