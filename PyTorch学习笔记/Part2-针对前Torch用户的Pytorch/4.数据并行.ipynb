{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多GPU样例\n",
    "==================\n",
    "\n",
    "数据并行是当我们将小批量的样品分割成更小的小批量，并行地运行每一个更小的小批量的计算。\n",
    "\n",
    "数据并行是使用``torch.nn.DataParallel``。\n",
    "你可以把一个模块包装在``DataParallel``里面，它将在批处理的维度上并行使用多个gpu。\n",
    "\n",
    "数据并行\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DataParallelModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Linear(10, 20)\n",
    "\n",
    "        # wrap block2 in DataParallel\n",
    "        self.block2 = nn.Linear(20, 20)\n",
    "        self.block2 = nn.DataParallel(self.block2)\n",
    "\n",
    "        self.block3 = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码不需要在CPU-mode中进行更改。\n",
    "DataParallel的文档<http://pytorch.org/docs/nn.html#dataparallel>\n",
    "\n",
    "**DataParallel实现的基元：**\n",
    "\n",
    "\n",
    "通常Pytorch的`nn.parallel`基元可以独立使用\n",
    "已经实现了简单的MPI-like基元：\n",
    "\n",
    "- replicate: 在多个设备上复制一个模块\n",
    "- scatter: 在第一维中分配输入\n",
    "- gather: 收集并连接第一维的输入\n",
    "- parallel\\_apply: 将一组已经分布的输入应用到一组已经分布的模型中\n",
    "\n",
    "为了让大家更清楚，函数``data_parallel``使用这些集合做成。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parallel(module, input, device_ids, output_device=None):\n",
    "    if not device_ids:\n",
    "        return module(input)\n",
    "\n",
    "    if output_device is None:\n",
    "        output_device = device_ids[0]\n",
    "\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "    inputs = nn.parallel.scatter(input, device_ids)\n",
    "    replicas = replicas[:len(inputs)]\n",
    "    outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "    return nn.parallel.gather(outputs, output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上模型的一部分和GPU上模型的一部分\n",
    "--------------------------------------------\n",
    "\n",
    "让我们来看一个实现网络的小例子，它的一部分在CPU上，一部分在GPU上\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class DistributedModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            embedding=nn.Embedding(1000, 10),\n",
    "            rnn=nn.Linear(10, 10).to(device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute embedding on CPU\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Transfer to GPU\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Compute RNN on GPU\n",
    "        x = self.rnn(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
