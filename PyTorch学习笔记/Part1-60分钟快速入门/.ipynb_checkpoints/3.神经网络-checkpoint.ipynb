{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "神经网络\n",
    "===============\n",
    "\n",
    "神经网络可以用``torch.nn``包来构建。\n",
    "\n",
    " ``nn``依赖于``autograd`` 定义模型并对其进行区分。\n",
    "一个 ``nn.Module`` 包含layers,和一个method ``forward(input)``用来返回``output``.\n",
    "\n",
    "以手写数字识别为例：\n",
    "<center><img src=\"img/mnist.png\" width=\"50%\"/></center>\n",
    "这是一个简单的前馈网络。它接受输入通过一些层，最终给出输出。\n",
    "\n",
    "神经网络的典型训练程序如下：\n",
    "\n",
    "- 定义具有可学习参数（或权重）的神经网络\n",
    "- 在一个输入的数据集上迭代\n",
    "- 通过网络处理输入\n",
    "- 计算损失（与正确的输出距离有多远）\n",
    "- 反向传播梯度给网络的参数\n",
    "- 更新网络的权重，通常使用一个简单的更新规则：\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "定义网络\n",
    "------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) #输入为1通道，输出为6通道，kernel size:5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) #最后一个池化层后为5x5x16的feature map，因此全连接层尺寸为（16x5x5,120）\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 尺寸是正方形，可以只指定一个数字\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 取除了batch之外的所有维度，因为二维卷积层输入矩阵的维度为 (N,Cin,H,W)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你只需要定义``forward``函数，``backward``函数 (用来计算梯度) 是自动为你定义的。\n",
    "你可以在``forward``函数中使用任何``Tensor``运算。\n",
    "\n",
    "模型的可学习参数由``net.parameters()``返回\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们尝试一个随机的32x32输入\n",
    "注意：这个网络的期望输入大小（LeNet）是32x32。要使用这个网络在MNIST数据集，请将数据调整到32x32。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0155,  0.0552,  0.0103, -0.1088, -0.0398, -0.0659, -0.0163,\n",
      "          0.0116, -0.1223, -0.0396]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有参数的梯度缓冲区和随机梯度置为0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn``只支持mini-batches. 整个``torch.nn``只支持输入为小批量样本,并且不能是单个样本.\n",
    "\n",
    "    比如, ``nn.Conv2d`` 接受一个4D的Tensor：\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "   如果只用一个样本,使用``input.unsqueeze(0)`` 添加一个假的batch维度.</p></div>\n",
    "\n",
    "在继续之前，回顾一下目前看到的所有类。\n",
    "\n",
    "**回顾:**\n",
    "  -  ``torch.Tensor`` - 一个*多维数组*支持autograd操作，例如``backward()``。也保存了关于tensor的梯度\n",
    "  -  ``nn.Module`` - 神经网络模块。 *封装参数的方便方法*, ：帮助转移到GPU，导出，加载等\n",
    "  -  ``nn.Parameter`` - 一种Tensor, 当把它赋值给一个Module时,被自动的注册为一个参数。\n",
    "  -  ``autograd.Function`` - 实现一个自动求导操作的前向和反向定义,每个tensor操作至少创建一个函数节点,它连接到产生一个tensor的函数上并对其历史进行编码\n",
    "\n",
    "**到这里，已经讨论了：**\n",
    "  -  定义一个神经网络\n",
    "  -  处理输入和调用backward\n",
    "\n",
    "**剩下的内容：**\n",
    "  -  计算损失值\n",
    "  -  更新神经网络的权值\n",
    "损失函数\n",
    "-------------\n",
    "一个损失函数接受一对`(output, target)`作为输入(output为网络的输出,target为实际值),计算一个值来估计网络的输出和目标值相差多少.\n",
    "\n",
    "在nn包中有几种不同的损失函数 <http://pytorch.org/docs/nn.html#loss-functions>。\n",
    "一个简单的损失函数是：``nn.MSELoss``：它计算网络预测和目标之间的均方误差。\n",
    "\n",
    "例如：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.9883)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1, 11)  # 一个虚拟目标\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在,你反向跟踪`loss`,使用它的`.grad_fn`属性,你会看到向下面这样的一个计算图:\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "所以, 当你调用 ``loss.backward()``, 整个图关于损失被求导，图中所有的Tensor的``requres_grad=True``\n",
    "所以``.grad`` Tensor累计它们的梯度。\n",
    "\n",
    "为了说明,我们反向跟踪几步:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f481005acc0>\n",
      "<AddmmBackward object at 0x7f481005ac88>\n",
      "<ExpandBackward object at 0x7f481005acc0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播\n",
    "--------\n",
    "为了反向传播误差,我们所需做的是调用``loss.backward()``。\n",
    "你需要清除已存在的梯度,否则梯度将被累加到已存在的梯度上。\n",
    "\n",
    "现在，将调用``loss.backward()``，并查看conv1层的偏置项在反向传播前后的梯度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.1015,  0.0653, -0.0098,  0.0378, -0.0393, -0.0104])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经看到了如何使用损失函数。\n",
    "\n",
    "**稍后阅读：**\n",
    "\n",
    " 神经网络包包含了各种用来构成深度神经网络的模块和损失函数,一份完整的文档查看[这里](http://pytorch.org/docs/nn>)。\n",
    "\n",
    "更新权重\n",
    "------------------\n",
    "实践中最简单的更新规则是随机梯度下降(SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "我们可以使用简单的Python代码实现这个规则：\n",
    "\n",
    "```python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "然而，当你使用神经网络是，你想要使用各种不同的更新规则，比如SGD，Nesterov-SGD，Adam，MSPROP等。为了做到这一点，构建了一个包``torch.optim``实现了所有的这些规则。使用他们非常简单：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
